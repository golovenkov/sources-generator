{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq char-level generative model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import math \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# based on https://github.com/IBM/pytorch-seq2seq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgeny/.local/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import logging\n",
    "\n",
    "import seq2seq\n",
    "from seq2seq.trainer import SupervisedTrainer\n",
    "from seq2seq.models import EncoderRNN, DecoderRNN, Seq2seq\n",
    "from seq2seq.loss import Perplexity\n",
    "from seq2seq.optim import Optimizer\n",
    "from seq2seq.dataset import SourceField, TargetField\n",
    "from seq2seq.evaluator import Predictor\n",
    "from seq2seq.util.checkpoint import Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FORMAT = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s'\n",
    "logging.basicConfig(format=LOG_FORMAT, level=getattr(logging, \"INFO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "src = SourceField(tokenize=lambda x: list(x))\n",
    "tgt = TargetField(tokenize=lambda x: list(x))\n",
    "max_len = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', ' ', 't', 'h', 'i', 'n', 'k', ' ', 's', 'o']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize example\n",
    "src.tokenize(\"i think so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_filter(example):\n",
    "    return len(example.src) <= max_len and len(example.tgt) <= max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./df_train.csv\"\n",
    "dev_path = \"./df_valid.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_train_dev_csv(directory='./sources/'):\n",
    "    # Get list of files\n",
    "    sourcefiles = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    # shuffle\n",
    "    random.shuffle(sourcefiles)\n",
    "    splt = int(len(sourcefiles)*0.95)\n",
    "    \n",
    "    dataset_dict = {}\n",
    "    samples_list = []\n",
    "    for filename in sourcefiles[:splt]:\n",
    "        with open(os.path.join(directory, filename), 'rt', encoding='utf-8', errors='ignore') as f:\n",
    "            samples_list.append(f.read().replace('\\x00', ''))\n",
    "    dataset_dict['src'] = samples_list\n",
    "    dataset_dict['tgt'] = samples_list\n",
    "    print(\"# train samples:\", len(samples_list))\n",
    "    df = pd.DataFrame.from_dict(dataset_dict)\n",
    "    df.to_csv(train_path, index=False)\n",
    "    \n",
    "    dataset_dict = {}\n",
    "    samples_list = []\n",
    "    for filename in sourcefiles[splt:]:\n",
    "        with open(os.path.join(directory, filename), 'rt', encoding='utf-8', errors='ignore') as f:\n",
    "            samples_list.append(f.read().replace('\\x00', ''))\n",
    "    dataset_dict['src'] = samples_list\n",
    "    dataset_dict['tgt'] = samples_list\n",
    "    print(\"# valid samples:\", len(samples_list))\n",
    "    df = pd.DataFrame.from_dict(dataset_dict)\n",
    "    df.to_csv(dev_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train samples: 20843\n",
      "# valid samples: 1098\n"
     ]
    }
   ],
   "source": [
    "format_train_dev_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src,tgt\r\n",
      "\"/* { dg-do run } */\r\n",
      "/* { dg-require-effective-target avx } */\r\n",
      "/* { dg-options \"\"-O2 -mfpmath=sse -mavx\"\" } */\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head df_train.csv -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchtext.data.TabularDataset(\n",
    "    path=train_path, format='csv',\n",
    "    fields=[('src', src), ('tgt', tgt)],\n",
    "    filter_pred=len_filter,\n",
    "    skip_header=True\n",
    ")\n",
    "dev = torchtext.data.TabularDataset(\n",
    "    path=dev_path, format='csv',\n",
    "    fields=[('src', src), ('tgt', tgt)],\n",
    "    filter_pred=len_filter,\n",
    "    skip_header=True\n",
    ")\n",
    "src.build_vocab(train)\n",
    "tgt.build_vocab(train)\n",
    "input_vocab = src.vocab\n",
    "output_vocab = tgt.vocab\n",
    "\n",
    "# NOTE: If the source field name and the target field name\n",
    "# are different from 'src' and 'tgt' respectively, they have\n",
    "# to be set explicitly before any training or inference\n",
    "# seq2seq.src_field_name = 'src'\n",
    "# seq2seq.tgt_field_name = 'tgt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/', '*', ' ', '{', ' ', 'd', 'g'], ['<sos>', '/', '*', ' ', '{', ' ', 'd'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.examples[0].src[:7], train.examples[0].tgt[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgeny/.local/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# Prepare loss\n",
    "weight = torch.ones(len(tgt.vocab))\n",
    "pad = tgt.vocab.stoi[tgt.pad_token]\n",
    "loss = Perplexity(weight, pad)\n",
    "if torch.cuda.is_available():\n",
    "    loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgeny/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "seq2seq = None\n",
    "optimizer = None\n",
    "if not resume:\n",
    "    # Initialize model\n",
    "    hidden_size = 128\n",
    "    bidirectional = True\n",
    "    encoder = EncoderRNN(len(src.vocab), max_len, hidden_size,\n",
    "                         bidirectional=bidirectional, variable_lengths=True)\n",
    "    decoder = DecoderRNN(len(tgt.vocab), max_len, hidden_size * 2 if bidirectional else hidden_size,\n",
    "                         dropout_p=0.2, use_attention=True, bidirectional=bidirectional,\n",
    "                         eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n",
    "    seq2seq = Seq2seq(encoder, decoder)\n",
    "    if torch.cuda.is_available():\n",
    "        seq2seq.cuda()\n",
    "\n",
    "    for param in seq2seq.parameters():\n",
    "        param.data.uniform_(-0.08, 0.08)\n",
    "\n",
    "    # Optimizer and learning rate scheduler can be customized by\n",
    "    # explicitly constructing the objects and pass to the trainer.\n",
    "    #\n",
    "    # optimizer = Optimizer(torch.optim.Adam(seq2seq.parameters()), max_grad_norm=5)\n",
    "    # scheduler = StepLR(optimizer.optimizer, 1)\n",
    "    # optimizer.set_scheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-09 10:54:50,692 seq2seq.trainer.supervised_trainer INFO     Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      "), Scheduler: None\n",
      "/home/evgeny/.local/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "2018-10-09 10:55:06,306 seq2seq.trainer.supervised_trainer INFO     Progress: 4%, Train Perplexity: 16.9279\n",
      "2018-10-09 10:55:11,557 seq2seq.trainer.supervised_trainer INFO     Progress: 6%, Train Perplexity: 9.9591\n",
      "2018-10-09 10:55:19,035 seq2seq.trainer.supervised_trainer INFO     Progress: 8%, Train Perplexity: 26.2419\n",
      "2018-10-09 10:55:26,871 seq2seq.trainer.supervised_trainer INFO     Progress: 10%, Train Perplexity: 29.3013\n",
      "2018-10-09 10:55:36,889 seq2seq.trainer.supervised_trainer INFO     Progress: 12%, Train Perplexity: 34.9097\n",
      "2018-10-09 10:55:41,646 seq2seq.trainer.supervised_trainer INFO     Progress: 14%, Train Perplexity: 11.9633\n",
      "2018-10-09 10:55:55,608 seq2seq.trainer.supervised_trainer INFO     Progress: 16%, Train Perplexity: 29.5773\n",
      "2018-10-09 10:56:01,625 seq2seq.trainer.supervised_trainer INFO     Progress: 18%, Train Perplexity: 3.3310\n",
      "2018-10-09 10:56:08,041 seq2seq.trainer.supervised_trainer INFO     Progress: 20%, Train Perplexity: 41.1955\n",
      "2018-10-09 10:56:17,795 seq2seq.trainer.supervised_trainer INFO     Progress: 22%, Train Perplexity: 82.3736\n",
      "2018-10-09 10:56:21,218 seq2seq.trainer.supervised_trainer INFO     Progress: 24%, Train Perplexity: 1.7385\n",
      "2018-10-09 10:56:37,804 seq2seq.trainer.supervised_trainer INFO     Progress: 26%, Train Perplexity: 163.1455\n",
      "2018-10-09 10:56:45,526 seq2seq.trainer.supervised_trainer INFO     Progress: 28%, Train Perplexity: 84.2039\n",
      "2018-10-09 10:56:58,632 seq2seq.trainer.supervised_trainer INFO     Progress: 30%, Train Perplexity: 121.2867\n",
      "2018-10-09 10:57:17,725 seq2seq.trainer.supervised_trainer INFO     Progress: 32%, Train Perplexity: 163.6294\n",
      "/home/evgeny/.local/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n",
      "/home/evgeny/.local/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n",
      "2018-10-09 10:57:29,030 seq2seq.trainer.supervised_trainer INFO     Finished epoch 1: Train Perplexity: 50.3169, Dev Perplexity: 205.8094, Accuracy: 0.2667\n",
      "2018-10-09 10:57:33,188 seq2seq.trainer.supervised_trainer INFO     Progress: 34%, Train Perplexity: 1.2825\n",
      "2018-10-09 10:57:39,276 seq2seq.trainer.supervised_trainer INFO     Progress: 36%, Train Perplexity: 63.6034\n",
      "2018-10-09 10:57:56,899 seq2seq.trainer.supervised_trainer INFO     Progress: 38%, Train Perplexity: 219.6344\n",
      "2018-10-09 10:58:01,600 seq2seq.trainer.supervised_trainer INFO     Progress: 40%, Train Perplexity: 1.1412\n",
      "2018-10-09 10:58:07,434 seq2seq.trainer.supervised_trainer INFO     Progress: 42%, Train Perplexity: 23.9763\n",
      "2018-10-09 10:58:15,677 seq2seq.trainer.supervised_trainer INFO     Progress: 44%, Train Perplexity: 81.1684\n",
      "2018-10-09 10:58:21,726 seq2seq.trainer.supervised_trainer INFO     Progress: 47%, Train Perplexity: 1.4763\n",
      "2018-10-09 10:58:31,051 seq2seq.trainer.supervised_trainer INFO     Progress: 49%, Train Perplexity: 105.7709\n",
      "2018-10-09 10:58:34,645 seq2seq.trainer.supervised_trainer INFO     Progress: 51%, Train Perplexity: 1.0617\n",
      "2018-10-09 10:58:45,922 seq2seq.trainer.supervised_trainer INFO     Progress: 53%, Train Perplexity: 91.8219\n",
      "2018-10-09 10:59:00,851 seq2seq.trainer.supervised_trainer INFO     Progress: 55%, Train Perplexity: 179.2155\n",
      "2018-10-09 10:59:08,781 seq2seq.trainer.supervised_trainer INFO     Progress: 57%, Train Perplexity: 43.5113\n",
      "2018-10-09 10:59:13,759 seq2seq.trainer.supervised_trainer INFO     Progress: 59%, Train Perplexity: 8.0872\n",
      "2018-10-09 10:59:32,811 seq2seq.trainer.supervised_trainer INFO     Progress: 61%, Train Perplexity: 147.6843\n",
      "2018-10-09 10:59:36,978 seq2seq.trainer.supervised_trainer INFO     Progress: 63%, Train Perplexity: 2.8329\n",
      "2018-10-09 10:59:42,508 seq2seq.trainer.supervised_trainer INFO     Progress: 65%, Train Perplexity: 3.3319\n",
      "2018-10-09 10:59:58,571 seq2seq.trainer.supervised_trainer INFO     Finished epoch 2: Train Perplexity: 61.3432, Dev Perplexity: 110.2982, Accuracy: 0.4750\n",
      "2018-10-09 11:00:01,880 seq2seq.trainer.supervised_trainer INFO     Progress: 67%, Train Perplexity: 25.4346\n",
      "2018-10-09 11:00:20,191 seq2seq.trainer.supervised_trainer INFO     Progress: 69%, Train Perplexity: 154.1128\n",
      "2018-10-09 11:00:35,442 seq2seq.trainer.supervised_trainer INFO     Progress: 71%, Train Perplexity: 67.7054\n",
      "2018-10-09 11:00:44,675 seq2seq.trainer.supervised_trainer INFO     Progress: 73%, Train Perplexity: 9.0947\n",
      "2018-10-09 11:00:47,915 seq2seq.trainer.supervised_trainer INFO     Progress: 75%, Train Perplexity: 1.2447\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "t = SupervisedTrainer(loss=loss, batch_size=128,\n",
    "                      checkpoint_every=1000,\n",
    "                      print_every=10, expt_dir=\"./export_dir\")\n",
    "\n",
    "seq2seq = t.train(seq2seq, train,\n",
    "                  num_epochs=3, dev_data=dev,\n",
    "                  optimizer=optimizer,\n",
    "                  teacher_forcing_ratio=0.8,\n",
    "                  resume=resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(seq2seq, input_vocab, output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'v', 'o', 'i', 'd', ' ', 'q', 'u', 'i', 'c'] ['p', 'i', '\\n', ' ', ' ', ' ', ' ', '}', '\\n', '}']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgeny/.local/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#oid quickSort(arr[], low, high)\n",
      "{\n",
      "    if (low < high)\n",
      "    {\n",
      "        /* pi is partitioning index, arr[pi] is now\n",
      "           at right place */\n",
      "        pi = partition(arr, low, high);\n",
      "\n",
      "    if (low < high)\n",
      "    {\n",
      "        /* pi is partitioning index, arr[pi] is now\n",
      "           at right place */\n",
      "        pi = partition(arr, low, high);\n",
      "\n",
      "    if (low < high)\n",
      "    {\n",
      "        /* pi is partitioning index, arr[pi] is now\n",
      "           at right place */\n",
      "        pi = partition(arr, low, high);\n",
      "\n",
      "    if (low < high)\n",
      "    {\n",
      "        /* pi is partitioning index, arr[pi] is now\n",
      "           at right place */\n",
      "        pi = partition(arr, low, high);\n",
      "\n",
      "    if (low < high)\n",
      "    {\n",
      "        /* pi is partitioning index, arr[pi] is now\n",
      "           at right place */\n",
      "        pi = partition(arr, low, high);\n",
      "\n",
      "    if (low < high)\n",
      "    {\n",
      "        /* pi is partitioning index, arr[pi] is now\n",
      "           at right place */\n",
      "        pi = partition(arr, low, high);\n",
      "\n",
      "    if (low < high)\n",
      "    {\n",
      "        /* pi is partitioning index, arr[pi] is now\n",
      "           at right pla\n"
     ]
    }
   ],
   "source": [
    "a = \"int main()\\n{int x;\\n}\\n\"; seq = list(a)\n",
    "a = \"\"\"\n",
    "void quickSort(arr[], low, high)\n",
    "{\n",
    "    if (low < high)\n",
    "    {\n",
    "        /* pi is partitioning index, arr[pi] is now\n",
    "           at right place */\n",
    "        pi = partition(arr, low, high);\n",
    "\n",
    "        quickSort(arr, low, pi - 1);  // Before pi\n",
    "        quickSort(arr, pi + 1, high); // After pi\n",
    "    }\n",
    "}\"\"\"; seq = list(a)\n",
    "# seq = train.examples[0].src\n",
    "print(seq[:10], seq[-10:])\n",
    "print(\"\".join(predictor.predict(seq)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
