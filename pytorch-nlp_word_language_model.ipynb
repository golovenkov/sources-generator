{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch-nlp word_language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import math \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "sequence_length = 35\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use torchnlp because it supports word-level encoding along with BPTT batch sampler\n",
    "from torchnlp.text_encoders import WhitespaceEncoder, StaticTokenizerEncoder\n",
    "from torchnlp.samplers import BPTTBatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sources_datasets(directory='./sources/'):\n",
    "    # Get list of files\n",
    "    sourcefiles = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    # shuffle\n",
    "    random.shuffle(sourcefiles)\n",
    "    train_dataset = \"\"\n",
    "    for filename in sourcefiles:\n",
    "        with open(os.path.join(directory, filename), 'rt', encoding='utf-8', errors='ignore') as f:\n",
    "            train_dataset += f.read()\n",
    "    splt = int(len(train_dataset)*0.95)\n",
    "    return train_dataset[:splt], train_dataset[splt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = make_sources_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9047442, 476182)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# characters\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = StaticTokenizerEncoder([train_dataset] + [valid_dataset], tokenize=lambda s: s.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131838"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique tokens\n",
    "encoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset using character-level encoder\n",
    "train_data = encoder.encode(train_dataset)\n",
    "val_data = encoder.encode(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 12, 16, 17, 18])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/* { dg-do run } */\\n\\nstruct A\\n{\\n  int i;\\n};\\n\\nstruct B\\n{\\n  struct A a[2];\\n};\\n\\nint i = 1;\\nstruct B b ='"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check tokenizer\n",
    "train_dataset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/* { dg-do run } */\\n\\nstruct A\\n{\\n  int i;\\n};\\n\\nstruct B\\n{\\n  struct A a[2];\\n};\\n\\nint'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([encoder.itos[idx] for idx in train_data[:15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1589408, 83590)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens\n",
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '</s>', '<s>', '<copy>', '/*', '{', 'dg-do', 'run', '}', '*/\\n\\nstruct', 'A\\n{\\n', '']\n"
     ]
    }
   ],
   "source": [
    "# sample tokens from vocab\n",
    "print(encoder.vocab[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samplers\n",
    "train_source_sampler, val_source_sampler = tuple(\n",
    "    [BPTTBatchSampler(d, sequence_length, batch_size, True, 'source') for d in (train_data, val_data)])\n",
    "\n",
    "train_target_sampler, val_target_sampler = tuple(\n",
    "    [BPTTBatchSampler(d, sequence_length, batch_size, True, 'target') for d in (train_data, val_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " [slice(0, 35, None), slice(79470, 79505, None), slice(158940, 158975, None)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num of samples in a batch\n",
    "len(next(iter(train_source_sampler))), next(iter(train_source_sampler))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2271"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num of batches\n",
    "len(train_source_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    5, 13898,    24,    12],\n",
      "        [    6,   393,  1019,    12],\n",
      "        [    7,    89,   445,    12],\n",
      "        [    8,    12,  2052, 32481],\n",
      "        [    9,    86,  1603,   304],\n",
      "        [   10,    48,  1161, 32482],\n",
      "        [   11,    12,  4444,    53],\n",
      "        [   12,    49, 23895, 32483],\n",
      "        [   13, 13899,  8889,    12],\n",
      "        [   14,  6867,    20,    40]])\n",
      "tensor([[    6,   393,  1019,    12],\n",
      "        [    7,    89,   445,    12],\n",
      "        [    8,    12,  2052, 32481],\n",
      "        [    9,    86,  1603,   304],\n",
      "        [   10,    48,  1161, 32482],\n",
      "        [   11,    12,  4444,    53],\n",
      "        [   12,    49, 23895, 32483],\n",
      "        [   13, 13899,  8889,    12],\n",
      "        [   14,  6867,    20,    40],\n",
      "        [   15,    89,  5208,    12]])\n"
     ]
    }
   ],
   "source": [
    "for source_sample, target_sample in zip(train_source_sampler, train_target_sampler):\n",
    "    print(torch.stack([train_data[i] for i in source_sample]).t_().contiguous()[:10, :4])\n",
    "    print(torch.stack([train_data[i] for i in target_sample]).t_().contiguous()[:10, :4])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/word_language_model/model.py\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source, source_sampler, target_sampler):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = encoder.vocab_size\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for source_sample, target_sample in zip(source_sampler, target_sampler):\n",
    "            data = torch.stack([data_source[i] for i in source_sample]).t_().contiguous().to(device)                # source chars\n",
    "            targets = torch.stack([data_source[i] for i in target_sample]).t_().contiguous().view(-1).to(device)    # target chars\n",
    "\n",
    "            output, hidden = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += criterion(output_flat, targets).item()\n",
    "    return total_loss / len(source_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_source, source_sampler, target_sampler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = encoder.vocab_size\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, (source_sample, target_sample) in enumerate(zip(source_sampler, target_sampler)):        \n",
    "        data = torch.stack([data_source[i] for i in source_sample]).t_().contiguous().to(device)               # source chars\n",
    "        targets = torch.stack([data_source[i] for i in target_sample]).t_().contiguous().view(-1).to(device)   # target chars\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(source_sampler), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens:  131838\n"
     ]
    }
   ],
   "source": [
    "ntokens = encoder.vocab_size; print(\"# tokens: \", ntokens)\n",
    "model = RNNModel('LSTM', ntokens, 1500, 1500, 2, 0.65, True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "grad_clip = 0.1\n",
    "lr = 20.\n",
    "best_val_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval() \n",
    "    hidden = model.init_hidden(bsz=1)   # init hidden state and cell state with zeros for batch_size = 1\n",
    "    x = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)   # pass random token\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        # output is a Tensor of shape [seq_len, batch_size, vocab_len] of tokens' probabilities\n",
    "        word_weights = output.squeeze().div(temp).exp().cpu()\n",
    "        # sample, then take the first element of the array\n",
    "        word_idx = torch.multinomial(input=word_weights, num_samples=1)[0] \n",
    "        x.fill_(word_idx)\n",
    "        word = encoder.itos[word_idx]\n",
    "        out.append(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " \"vfnmsub\\[123\\]+sd\" bar[j];\n",
      "}\n",
      "/* count)\t\\\n",
      "{\t\t\t\t\t\t\t\\\n",
      " hfa_union_t, \"12345\";\n",
      "char mysin;\n",
      "double restoring tree-optimization/83338 \"mov (pool_v4sf) \"pmaxuw\" ++out)\n",
      " .global\\[^,\\n\\r\\]*external_decl\" {0x5414}, inc19 run_expensive_tests middle-end/40340 move_epi64 *result_type, T\n",
      "#define\n",
      "\"nmachhw\\\\. IACC c[1024];\n",
      "int mantissa1:32;\n",
      "} }));\t\\\n",
      " (5)), x9 __attribute__((noinline,noclone))\n",
      "badfunc(int max_h_samp_factor;\n",
      " z(int);\n",
      "int -mfpu=fpv4-sp-d16\" \"incssp\\[dq]\" short,\n",
      "\t\t bar(1);\n",
      " (one, --i); p3);\n",
      "}\n",
      "extern 3824. 20th (bfd_link_executable\n",
      "must_annul)\n",
      " */\n",
      "{\n",
      "\tbig();\n",
      "\tbig();\n",
      "\tbig();\n",
      "\tbig();\n",
      "\tbig();\n",
      "\tbig();\n",
      "\tbig();\n",
      "\tbig();\n",
      "\tbig();\n",
      "\tbig();\n",
      "}\n",
      "int __attribute__((malloc,transaction_safe));\n",
      "\n",
      "vectype c++/38410.\n",
      " d++)\n",
      " \"-march=armv7-a+fp if(a++)\n",
      " \"xvxsigdp\" c);\n",
      "}\n",
      "\n",
      "double sq2;\n",
      "_Sat  \n",
      "\n",
      "| epoch   1 |   100/ 2271 batches | lr 20.00 | loss  8.62 | ppl  5551.76\n",
      "| epoch   1 |   200/ 2271 batches | lr 20.00 | loss  6.93 | ppl  1026.55\n",
      "| epoch   1 |   300/ 2271 batches | lr 20.00 | loss  6.32 | ppl   552.82\n",
      "| epoch   1 |   400/ 2271 batches | lr 20.00 | loss  6.05 | ppl   423.01\n",
      "| epoch   1 |   500/ 2271 batches | lr 20.00 | loss  5.67 | ppl   290.29\n",
      "| epoch   1 |   600/ 2271 batches | lr 20.00 | loss  5.49 | ppl   243.28\n",
      "| epoch   1 |   700/ 2271 batches | lr 20.00 | loss  5.30 | ppl   199.96\n",
      "| epoch   1 |   800/ 2271 batches | lr 20.00 | loss  5.13 | ppl   169.59\n",
      "| epoch   1 |   900/ 2271 batches | lr 20.00 | loss  5.05 | ppl   156.75\n",
      "| epoch   1 |  1000/ 2271 batches | lr 20.00 | loss  4.97 | ppl   143.73\n",
      "| epoch   1 |  1100/ 2271 batches | lr 20.00 | loss  4.92 | ppl   136.64\n",
      "| epoch   1 |  1200/ 2271 batches | lr 20.00 | loss  4.79 | ppl   120.49\n",
      "| epoch   1 |  1300/ 2271 batches | lr 20.00 | loss  4.81 | ppl   123.03\n",
      "| epoch   1 |  1400/ 2271 batches | lr 20.00 | loss  4.61 | ppl   100.45\n",
      "| epoch   1 |  1500/ 2271 batches | lr 20.00 | loss  4.62 | ppl   101.83\n",
      "| epoch   1 |  1600/ 2271 batches | lr 20.00 | loss  4.65 | ppl   105.07\n",
      "| epoch   1 |  1700/ 2271 batches | lr 20.00 | loss  4.62 | ppl   101.39\n",
      "| epoch   1 |  1800/ 2271 batches | lr 20.00 | loss  4.47 | ppl    87.26\n",
      "| epoch   1 |  1900/ 2271 batches | lr 20.00 | loss  4.44 | ppl    84.40\n",
      "| epoch   1 |  2000/ 2271 batches | lr 20.00 | loss  4.47 | ppl    87.53\n",
      "| epoch   1 |  2100/ 2271 batches | lr 20.00 | loss  4.44 | ppl    84.74\n",
      "| epoch   1 |  2200/ 2271 batches | lr 20.00 | loss  4.34 | ppl    76.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  4.17 | valid ppl    64.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " (e1);\n",
      " (g); * \"../../aarch64/simd/vrev64qs32.x\"\n",
      "\n",
      "/* { be scan-rtl-dump \"0x\\[0-9a-f\\]+ may be for flexible 48:\n",
      " (a+b+c+d+e+f+g+h+i+j+k+l+m+n);\n",
      "}\n",
      "\n",
      "/* } specqsort(base, += i<128; + (16)));\n",
      "\n",
      "extern\n",
      "y if *q = CR4, PR =retme */\n",
      "\n",
      "extern void abort (*y void u = 4)\n",
      "\n",
      "#define i)\n",
      "{\n",
      "  c = _mm_cvtpi16_ps\n",
      "(unsigned int *b = \"\\tbl\\t__.*hf3\"  volatile int vec_abs   \n",
      "\n",
      "| epoch   2 |   100/ 2271 batches | lr 20.00 | loss  4.30 | ppl    73.59\n",
      "| epoch   2 |   200/ 2271 batches | lr 20.00 | loss  4.23 | ppl    68.94\n",
      "| epoch   2 |   300/ 2271 batches | lr 20.00 | loss  4.16 | ppl    64.15\n",
      "| epoch   2 |   400/ 2271 batches | lr 20.00 | loss  4.23 | ppl    68.79\n",
      "| epoch   2 |   500/ 2271 batches | lr 20.00 | loss  4.11 | ppl    61.18\n",
      "| epoch   2 |   600/ 2271 batches | lr 20.00 | loss  4.13 | ppl    61.98\n",
      "| epoch   2 |   700/ 2271 batches | lr 20.00 | loss  4.08 | ppl    59.21\n",
      "| epoch   2 |   800/ 2271 batches | lr 20.00 | loss  4.06 | ppl    57.79\n",
      "| epoch   2 |   900/ 2271 batches | lr 20.00 | loss  4.05 | ppl    57.28\n",
      "| epoch   2 |  1000/ 2271 batches | lr 20.00 | loss  4.05 | ppl    57.54\n",
      "| epoch   2 |  1100/ 2271 batches | lr 20.00 | loss  4.04 | ppl    56.55\n",
      "| epoch   2 |  1200/ 2271 batches | lr 20.00 | loss  4.00 | ppl    54.34\n",
      "| epoch   2 |  1300/ 2271 batches | lr 20.00 | loss  4.06 | ppl    57.83\n",
      "| epoch   2 |  1400/ 2271 batches | lr 20.00 | loss  3.88 | ppl    48.64\n",
      "| epoch   2 |  1500/ 2271 batches | lr 20.00 | loss  3.92 | ppl    50.24\n",
      "| epoch   2 |  1600/ 2271 batches | lr 20.00 | loss  4.00 | ppl    54.72\n",
      "| epoch   2 |  1700/ 2271 batches | lr 20.00 | loss  3.98 | ppl    53.71\n",
      "| epoch   2 |  1800/ 2271 batches | lr 20.00 | loss  3.89 | ppl    48.91\n",
      "| epoch   2 |  1900/ 2271 batches | lr 20.00 | loss  3.90 | ppl    49.17\n",
      "| epoch   2 |  2000/ 2271 batches | lr 20.00 | loss  3.92 | ppl    50.53\n",
      "| epoch   2 |  2100/ 2271 batches | lr 20.00 | loss  3.91 | ppl    50.11\n",
      "| epoch   2 |  2200/ 2271 batches | lr 20.00 | loss  3.81 | ppl    45.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  3.72 | valid ppl    41.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "   i = \"\\tvcvtb.f16.f32\" (6, testll_ r->a++;\n",
      "  y;}\n",
      "/* %0, \"jnz 0);\n",
      "  if (check_union256i_d last--) (vd != arm_arch_v5t_ok}\n",
      "   abort (0);\n",
      "  exit (0);\n",
      "}\n",
      "/* PR vec_sqrt(a);\n",
      "}\n",
      "struct  */\n",
      "\n",
      "#define DEBUG 0\n",
      "#if __attribute__((vector_size(4)));\n",
      "\n",
      "vec8 DEBUG\n",
      "#include MAX S, 1, 3,\n",
      "a0; i)\n",
      "{\n",
      "  static s = { 1, 2, 3,  \n",
      "\n",
      "| epoch   3 |   100/ 2271 batches | lr 20.00 | loss  3.81 | ppl    45.09\n",
      "| epoch   3 |   200/ 2271 batches | lr 20.00 | loss  3.76 | ppl    43.09\n",
      "| epoch   3 |   300/ 2271 batches | lr 20.00 | loss  3.71 | ppl    40.78\n",
      "| epoch   3 |   400/ 2271 batches | lr 20.00 | loss  3.78 | ppl    43.82\n",
      "| epoch   3 |   500/ 2271 batches | lr 20.00 | loss  3.67 | ppl    39.38\n",
      "| epoch   3 |   600/ 2271 batches | lr 20.00 | loss  3.70 | ppl    40.65\n",
      "| epoch   3 |   700/ 2271 batches | lr 20.00 | loss  3.68 | ppl    39.47\n",
      "| epoch   3 |   800/ 2271 batches | lr 20.00 | loss  3.67 | ppl    39.28\n",
      "| epoch   3 |   900/ 2271 batches | lr 20.00 | loss  3.67 | ppl    39.09\n",
      "| epoch   3 |  1000/ 2271 batches | lr 20.00 | loss  3.68 | ppl    39.63\n",
      "| epoch   3 |  1100/ 2271 batches | lr 20.00 | loss  3.66 | ppl    39.05\n",
      "| epoch   3 |  1200/ 2271 batches | lr 20.00 | loss  3.64 | ppl    38.24\n",
      "| epoch   3 |  1300/ 2271 batches | lr 20.00 | loss  3.71 | ppl    40.75\n",
      "| epoch   3 |  1400/ 2271 batches | lr 20.00 | loss  3.55 | ppl    34.92\n",
      "| epoch   3 |  1500/ 2271 batches | lr 20.00 | loss  3.58 | ppl    35.82\n",
      "| epoch   3 |  1600/ 2271 batches | lr 20.00 | loss  3.67 | ppl    39.27\n",
      "| epoch   3 |  1700/ 2271 batches | lr 20.00 | loss  3.65 | ppl    38.54\n",
      "| epoch   3 |  1800/ 2271 batches | lr 20.00 | loss  3.59 | ppl    36.12\n",
      "| epoch   3 |  1900/ 2271 batches | lr 20.00 | loss  3.60 | ppl    36.62\n",
      "| epoch   3 |  2000/ 2271 batches | lr 20.00 | loss  3.61 | ppl    37.06\n",
      "| epoch   3 |  2100/ 2271 batches | lr 20.00 | loss  3.61 | ppl    37.02\n",
      "| epoch   3 |  2200/ 2271 batches | lr 20.00 | loss  3.50 | ppl    33.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  3.49 | valid ppl    32.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " { dg-do compile } */\n",
      "/* { dg-options \"-O2 \\\\\\[sp, -mlong64 -fno-tree-dominator-opts __attribute__((altivec(vector__))) } */\n",
      "\n",
      "int bar (int i)\n",
      "{\n",
      "  return (a\n",
      "& 0.f << tem2;\n",
      "}\n",
      "\n",
      "int\n",
      "fn7 (long pixel long k[0];\n",
      " long long _Fract y)\n",
      "{\n",
      "  return (y ? (long long) PROT_READ <<\n",
      "x;\n",
      "}\n",
      "\n",
      "NOMIPS16 a.ab.b;\n",
      " y, - \"abc\" }\n",
      "NOMIPS16 / list_head\n",
      "{\n",
      " int u[i];\n",
      "  \n",
      "\n",
      "| epoch   4 |   100/ 2271 batches | lr 20.00 | loss  3.52 | ppl    33.81\n",
      "| epoch   4 |   200/ 2271 batches | lr 20.00 | loss  3.48 | ppl    32.61\n",
      "| epoch   4 |   300/ 2271 batches | lr 20.00 | loss  3.43 | ppl    30.82\n",
      "| epoch   4 |   400/ 2271 batches | lr 20.00 | loss  3.50 | ppl    32.96\n",
      "| epoch   4 |   500/ 2271 batches | lr 20.00 | loss  3.39 | ppl    29.77\n",
      "| epoch   4 |   600/ 2271 batches | lr 20.00 | loss  3.43 | ppl    30.94\n",
      "| epoch   4 |   700/ 2271 batches | lr 20.00 | loss  3.41 | ppl    30.39\n",
      "| epoch   4 |   800/ 2271 batches | lr 20.00 | loss  3.42 | ppl    30.55\n",
      "| epoch   4 |   900/ 2271 batches | lr 20.00 | loss  3.42 | ppl    30.52\n",
      "| epoch   4 |  1000/ 2271 batches | lr 20.00 | loss  3.42 | ppl    30.65\n",
      "| epoch   4 |  1100/ 2271 batches | lr 20.00 | loss  3.41 | ppl    30.17\n",
      "| epoch   4 |  1200/ 2271 batches | lr 20.00 | loss  3.41 | ppl    30.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1300/ 2271 batches | lr 20.00 | loss  3.47 | ppl    32.10\n",
      "| epoch   4 |  1400/ 2271 batches | lr 20.00 | loss  3.32 | ppl    27.61\n",
      "| epoch   4 |  1500/ 2271 batches | lr 20.00 | loss  3.35 | ppl    28.40\n",
      "| epoch   4 |  1600/ 2271 batches | lr 20.00 | loss  3.43 | ppl    30.84\n",
      "| epoch   4 |  1700/ 2271 batches | lr 20.00 | loss  3.42 | ppl    30.48\n",
      "| epoch   4 |  1800/ 2271 batches | lr 20.00 | loss  3.36 | ppl    28.90\n",
      "| epoch   4 |  1900/ 2271 batches | lr 20.00 | loss  3.38 | ppl    29.35\n",
      "| epoch   4 |  2000/ 2271 batches | lr 20.00 | loss  3.39 | ppl    29.74\n",
      "| epoch   4 |  2100/ 2271 batches | lr 20.00 | loss  3.39 | ppl    29.73\n",
      "| epoch   4 |  2200/ 2271 batches | lr 20.00 | loss  3.29 | ppl    26.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  3.33 | valid ppl    27.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " } */\n",
      "\n",
      "extern void bar (void);\n",
      "\n",
      "int\n",
      "foo (int (&x[0]));\n",
      "  {\n",
      "\tif (__builtin_constant_p 0))\n",
      "    {\n",
      "     \n",
      "int break;\n",
      "      double c; /* { \"avx512f-vpmovsxbd-2.c\"\n",
      "\n",
      "#undef ((externally_visible));\n",
      "int\n",
      "main()\n",
      "{\n",
      "}\n",
      "/* e;\n",
      "      j++;\n",
      "\n",
      " \"avx512f-vpmovqd-2.c\"\n",
      "\n",
      "#undef (TextWidget) \"umlal\"  ;\n",
      "  return b;\n",
      "}\n",
      "\n",
      "int main()\n",
      "{\n",
      "  \n",
      "\n",
      "| epoch   5 |   100/ 2271 batches | lr 20.00 | loss  3.31 | ppl    27.35\n",
      "| epoch   5 |   200/ 2271 batches | lr 20.00 | loss  3.27 | ppl    26.21\n",
      "| epoch   5 |   300/ 2271 batches | lr 20.00 | loss  3.22 | ppl    24.99\n",
      "| epoch   5 |   400/ 2271 batches | lr 20.00 | loss  3.28 | ppl    26.53\n",
      "| epoch   5 |   500/ 2271 batches | lr 20.00 | loss  3.19 | ppl    24.19\n",
      "| epoch   5 |   600/ 2271 batches | lr 20.00 | loss  3.23 | ppl    25.20\n",
      "| epoch   5 |   700/ 2271 batches | lr 20.00 | loss  3.21 | ppl    24.88\n",
      "| epoch   5 |   800/ 2271 batches | lr 20.00 | loss  3.21 | ppl    24.88\n",
      "| epoch   5 |   900/ 2271 batches | lr 20.00 | loss  3.22 | ppl    24.95\n",
      "| epoch   5 |  1000/ 2271 batches | lr 20.00 | loss  3.22 | ppl    25.15\n",
      "| epoch   5 |  1100/ 2271 batches | lr 20.00 | loss  3.21 | ppl    24.86\n",
      "| epoch   5 |  1200/ 2271 batches | lr 20.00 | loss  3.22 | ppl    25.13\n",
      "| epoch   5 |  1300/ 2271 batches | lr 20.00 | loss  3.27 | ppl    26.25\n",
      "| epoch   5 |  1400/ 2271 batches | lr 20.00 | loss  3.13 | ppl    22.83\n",
      "| epoch   5 |  1500/ 2271 batches | lr 20.00 | loss  3.16 | ppl    23.62\n",
      "| epoch   5 |  1600/ 2271 batches | lr 20.00 | loss  3.24 | ppl    25.44\n",
      "| epoch   5 |  1700/ 2271 batches | lr 20.00 | loss  3.22 | ppl    25.06\n",
      "| epoch   5 |  1800/ 2271 batches | lr 20.00 | loss  3.18 | ppl    24.13\n",
      "| epoch   5 |  1900/ 2271 batches | lr 20.00 | loss  3.21 | ppl    24.73\n",
      "| epoch   5 |  2000/ 2271 batches | lr 20.00 | loss  3.20 | ppl    24.65\n",
      "| epoch   5 |  2100/ 2271 batches | lr 20.00 | loss  3.21 | ppl    24.81\n",
      "| epoch   5 |  2200/ 2271 batches | lr 20.00 | loss  3.11 | ppl    22.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  3.22 | valid ppl    24.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  if vect_unaligned_possible != 2147483647) || 0;\n",
      "\n",
      "    abort ();\n",
      "}\n",
      "\n",
      "static inline int j **c;\n",
      "}\n",
      "// inline unsigned \"movups\\[\\\\t const\n",
      "int\n",
      "foo (void)\n",
      "{\n",
      "  unsigned char u3, ((13 = baz3 ()\n",
      "{\n",
      "  int *q;\n",
      "\n",
      "  strcpy d;)\n",
      "  (char, d, e;\n",
      "\n",
      "\n",
      " for (i = 0; i < sizeof (int) *  \n",
      "\n",
      "| epoch   6 |   100/ 2271 batches | lr 20.00 | loss  3.13 | ppl    22.86\n",
      "| epoch   6 |   200/ 2271 batches | lr 20.00 | loss  3.09 | ppl    21.96\n",
      "| epoch   6 |   300/ 2271 batches | lr 20.00 | loss  3.05 | ppl    21.01\n",
      "| epoch   6 |   400/ 2271 batches | lr 20.00 | loss  3.11 | ppl    22.38\n",
      "| epoch   6 |   500/ 2271 batches | lr 20.00 | loss  3.02 | ppl    20.44\n",
      "| epoch   6 |   600/ 2271 batches | lr 20.00 | loss  3.06 | ppl    21.23\n",
      "| epoch   6 |   700/ 2271 batches | lr 20.00 | loss  3.05 | ppl    21.10\n",
      "| epoch   6 |   800/ 2271 batches | lr 20.00 | loss  3.06 | ppl    21.32\n",
      "| epoch   6 |   900/ 2271 batches | lr 20.00 | loss  3.06 | ppl    21.26\n",
      "| epoch   6 |  1000/ 2271 batches | lr 20.00 | loss  3.06 | ppl    21.33\n",
      "| epoch   6 |  1100/ 2271 batches | lr 20.00 | loss  3.06 | ppl    21.32\n",
      "| epoch   6 |  1200/ 2271 batches | lr 20.00 | loss  3.07 | ppl    21.52\n",
      "| epoch   6 |  1300/ 2271 batches | lr 20.00 | loss  3.11 | ppl    22.36\n",
      "| epoch   6 |  1400/ 2271 batches | lr 20.00 | loss  2.97 | ppl    19.46\n",
      "| epoch   6 |  1500/ 2271 batches | lr 20.00 | loss  3.00 | ppl    20.15\n",
      "| epoch   6 |  1600/ 2271 batches | lr 20.00 | loss  3.07 | ppl    21.58\n",
      "| epoch   6 |  1700/ 2271 batches | lr 20.00 | loss  3.06 | ppl    21.42\n",
      "| epoch   6 |  1800/ 2271 batches | lr 20.00 | loss  3.03 | ppl    20.78\n",
      "| epoch   6 |  1900/ 2271 batches | lr 20.00 | loss  3.06 | ppl    21.38\n",
      "| epoch   6 |  2000/ 2271 batches | lr 20.00 | loss  3.05 | ppl    21.17\n",
      "| epoch   6 |  2100/ 2271 batches | lr 20.00 | loss  3.06 | ppl    21.25\n",
      "| epoch   6 |  2200/ 2271 batches | lr 20.00 | loss  2.96 | ppl    19.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss  3.13 | valid ppl    22.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " { dg-options int\n",
      "test_1 } */\n",
      "\n",
      "int f(int i, int j)\n",
      "{\n",
      "  (AVX512F_LEN, (i);\n",
      "  for \\t\\]and = 0; p; ++j)\n",
      " \n",
      "  (*((type *) f);\n",
      "  return 0;\n",
      "}\n",
      "/* { dg-do compile } */\n",
      "/* { dg-options f1(void)\n",
      "{\n",
      " (ac);\n",
      "}\n",
      "/* --param &f.i) 64-bit\n",
      "\n",
      "plugin moves */\n",
      "/* { dg-do compile { target { {  \n",
      "\n",
      "| epoch   7 |   100/ 2271 batches | lr 20.00 | loss  2.98 | ppl    19.66\n",
      "| epoch   7 |   200/ 2271 batches | lr 20.00 | loss  2.94 | ppl    18.89\n",
      "| epoch   7 |   300/ 2271 batches | lr 20.00 | loss  2.91 | ppl    18.33\n",
      "| epoch   7 |   400/ 2271 batches | lr 20.00 | loss  2.96 | ppl    19.20\n",
      "| epoch   7 |   500/ 2271 batches | lr 20.00 | loss  2.87 | ppl    17.68\n",
      "| epoch   7 |   600/ 2271 batches | lr 20.00 | loss  2.91 | ppl    18.44\n",
      "| epoch   7 |   700/ 2271 batches | lr 20.00 | loss  2.91 | ppl    18.27\n",
      "| epoch   7 |   800/ 2271 batches | lr 20.00 | loss  2.92 | ppl    18.51\n",
      "| epoch   7 |   900/ 2271 batches | lr 20.00 | loss  2.93 | ppl    18.66\n",
      "| epoch   7 |  1000/ 2271 batches | lr 20.00 | loss  2.92 | ppl    18.51\n",
      "| epoch   7 |  1100/ 2271 batches | lr 20.00 | loss  2.92 | ppl    18.52\n",
      "| epoch   7 |  1200/ 2271 batches | lr 20.00 | loss  2.93 | ppl    18.71\n",
      "| epoch   7 |  1300/ 2271 batches | lr 20.00 | loss  2.96 | ppl    19.24\n",
      "| epoch   7 |  1400/ 2271 batches | lr 20.00 | loss  2.83 | ppl    16.91\n",
      "| epoch   7 |  1500/ 2271 batches | lr 20.00 | loss  2.88 | ppl    17.83\n",
      "| epoch   7 |  1600/ 2271 batches | lr 20.00 | loss  2.93 | ppl    18.77\n",
      "| epoch   7 |  1700/ 2271 batches | lr 20.00 | loss  2.93 | ppl    18.64\n",
      "| epoch   7 |  1800/ 2271 batches | lr 20.00 | loss  2.90 | ppl    18.21\n",
      "| epoch   7 |  1900/ 2271 batches | lr 20.00 | loss  2.93 | ppl    18.81\n",
      "| epoch   7 |  2000/ 2271 batches | lr 20.00 | loss  2.91 | ppl    18.42\n",
      "| epoch   7 |  2100/ 2271 batches | lr 20.00 | loss  2.92 | ppl    18.53\n",
      "| epoch   7 |  2200/ 2271 batches | lr 20.00 | loss  2.83 | ppl    16.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss  3.07 | valid ppl    21.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " test1(0x0000000101);\n",
      "   \\\n",
      "   length i); /* { dg-warning \"statement as fall mode reduction\" } */\n",
      "  ((char\n",
      "*) f2 (__dest, (*fptr) (y, 0, 5);\n",
      "}\n",
      "/* { dg-do compile } */\n",
      "/* { dg-additional-options \"pr85056a.c\" } */\n",
      "\n",
      "/* Check that the\n",
      "passing properly general is generated with higher functions.  */\n",
      "\n",
      "/*  \n",
      "\n",
      "| epoch   8 |   100/ 2271 batches | lr 20.00 | loss  2.85 | ppl    17.21\n",
      "| epoch   8 |   200/ 2271 batches | lr 20.00 | loss  2.80 | ppl    16.53\n",
      "| epoch   8 |   300/ 2271 batches | lr 20.00 | loss  2.78 | ppl    16.07\n",
      "| epoch   8 |   400/ 2271 batches | lr 20.00 | loss  2.83 | ppl    16.93\n",
      "| epoch   8 |   500/ 2271 batches | lr 20.00 | loss  2.75 | ppl    15.67\n",
      "| epoch   8 |   600/ 2271 batches | lr 20.00 | loss  2.78 | ppl    16.18\n",
      "| epoch   8 |   700/ 2271 batches | lr 20.00 | loss  2.78 | ppl    16.13\n",
      "| epoch   8 |   800/ 2271 batches | lr 20.00 | loss  2.79 | ppl    16.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 |   900/ 2271 batches | lr 20.00 | loss  2.80 | ppl    16.52\n",
      "| epoch   8 |  1000/ 2271 batches | lr 20.00 | loss  2.80 | ppl    16.42\n",
      "| epoch   8 |  1100/ 2271 batches | lr 20.00 | loss  2.80 | ppl    16.37\n",
      "| epoch   8 |  1200/ 2271 batches | lr 20.00 | loss  2.81 | ppl    16.65\n",
      "| epoch   8 |  1300/ 2271 batches | lr 20.00 | loss  2.84 | ppl    17.18\n",
      "| epoch   8 |  1400/ 2271 batches | lr 20.00 | loss  2.70 | ppl    14.91\n",
      "| epoch   8 |  1500/ 2271 batches | lr 20.00 | loss  2.76 | ppl    15.74\n",
      "| epoch   8 |  1600/ 2271 batches | lr 20.00 | loss  2.81 | ppl    16.55\n",
      "| epoch   8 |  1700/ 2271 batches | lr 20.00 | loss  2.81 | ppl    16.62\n",
      "| epoch   8 |  1800/ 2271 batches | lr 20.00 | loss  2.79 | ppl    16.20\n",
      "| epoch   8 |  1900/ 2271 batches | lr 20.00 | loss  2.82 | ppl    16.78\n",
      "| epoch   8 |  2000/ 2271 batches | lr 20.00 | loss  2.80 | ppl    16.43\n",
      "| epoch   8 |  2100/ 2271 batches | lr 20.00 | loss  2.81 | ppl    16.57\n",
      "| epoch   8 |  2200/ 2271 batches | lr 20.00 | loss  2.72 | ppl    15.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss  3.01 | valid ppl    20.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  */\n",
      "\n",
      "/* { dg-do compile } */\n",
      "/* { dg-options \"-O2\" } conflicting IN_FRAMEWORK\n",
      "#define VFP\n",
      "#define TESTFILE a7, \"neon-constants.h\"\n",
      "\n",
      "\n",
      "#include x = 1.0+2.0i;\n",
      "\n",
      "struct\n",
      "y\n",
      "{\n",
      "  int p;\n",
      "  int q;\n",
      "  int r;\n",
      "  int r;\n",
      "  int r;\n",
      "  int r;\n",
      " \n",
      "int s;\n",
      "} v = { 1, 2, 3, 4 };\n",
      "\n",
      "struct  \n",
      "\n",
      "| epoch   9 |   100/ 2271 batches | lr 20.00 | loss  2.73 | ppl    15.29\n",
      "| epoch   9 |   200/ 2271 batches | lr 20.00 | loss  2.69 | ppl    14.78\n",
      "| epoch   9 |   300/ 2271 batches | lr 20.00 | loss  2.67 | ppl    14.44\n",
      "| epoch   9 |   400/ 2271 batches | lr 20.00 | loss  2.70 | ppl    14.92\n",
      "| epoch   9 |   500/ 2271 batches | lr 20.00 | loss  2.65 | ppl    14.09\n",
      "| epoch   9 |   600/ 2271 batches | lr 20.00 | loss  2.67 | ppl    14.45\n",
      "| epoch   9 |   700/ 2271 batches | lr 20.00 | loss  2.66 | ppl    14.36\n",
      "| epoch   9 |   800/ 2271 batches | lr 20.00 | loss  2.68 | ppl    14.61\n",
      "| epoch   9 |   900/ 2271 batches | lr 20.00 | loss  2.70 | ppl    14.90\n",
      "| epoch   9 |  1000/ 2271 batches | lr 20.00 | loss  2.69 | ppl    14.66\n",
      "| epoch   9 |  1100/ 2271 batches | lr 20.00 | loss  2.69 | ppl    14.68\n",
      "| epoch   9 |  1200/ 2271 batches | lr 20.00 | loss  2.70 | ppl    14.91\n",
      "| epoch   9 |  1300/ 2271 batches | lr 20.00 | loss  2.73 | ppl    15.31\n",
      "| epoch   9 |  1400/ 2271 batches | lr 20.00 | loss  2.60 | ppl    13.50\n",
      "| epoch   9 |  1500/ 2271 batches | lr 20.00 | loss  2.65 | ppl    14.13\n",
      "| epoch   9 |  1600/ 2271 batches | lr 20.00 | loss  2.71 | ppl    15.01\n",
      "| epoch   9 |  1700/ 2271 batches | lr 20.00 | loss  2.69 | ppl    14.79\n",
      "| epoch   9 |  1800/ 2271 batches | lr 20.00 | loss  2.68 | ppl    14.62\n",
      "| epoch   9 |  1900/ 2271 batches | lr 20.00 | loss  2.70 | ppl    14.91\n",
      "| epoch   9 |  2000/ 2271 batches | lr 20.00 | loss  2.69 | ppl    14.80\n",
      "| epoch   9 |  2100/ 2271 batches | lr 20.00 | loss  2.69 | ppl    14.80\n",
      "| epoch   9 |  2200/ 2271 batches | lr 20.00 | loss  2.61 | ppl    13.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss  2.97 | valid ppl    19.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " } */\n",
      "\n",
      "/* This test should succeed on both 32- and 64-bit configurations.  */\n",
      "#include <altivec.h>\n",
      "\n",
      "__vector unsigned sd;\n",
      "volatile (__vector unsigned int\n",
      "*p,\n",
      "\t\t\t\t   __vector unsigned int *q)\n",
      "{\n",
      "  __vector unsigned long int source_1, source_1, source_2;\n",
      "\n",
      "  source_1 = *p;\n",
      " \n",
      "source_2 = *q;\n",
      "\n",
      "  result = vec_absd (source_1, source_2);\n",
      "   \n",
      "\n",
      "| epoch  10 |   100/ 2271 batches | lr 20.00 | loss  2.63 | ppl    13.86\n",
      "| epoch  10 |   200/ 2271 batches | lr 20.00 | loss  2.59 | ppl    13.29\n",
      "| epoch  10 |   300/ 2271 batches | lr 20.00 | loss  2.56 | ppl    12.97\n",
      "| epoch  10 |   400/ 2271 batches | lr 20.00 | loss  2.61 | ppl    13.57\n",
      "| epoch  10 |   500/ 2271 batches | lr 20.00 | loss  2.54 | ppl    12.66\n",
      "| epoch  10 |   600/ 2271 batches | lr 20.00 | loss  2.57 | ppl    13.05\n",
      "| epoch  10 |   700/ 2271 batches | lr 20.00 | loss  2.56 | ppl    12.97\n",
      "| epoch  10 |   800/ 2271 batches | lr 20.00 | loss  2.58 | ppl    13.16\n",
      "| epoch  10 |   900/ 2271 batches | lr 20.00 | loss  2.60 | ppl    13.43\n",
      "| epoch  10 |  1000/ 2271 batches | lr 20.00 | loss  2.59 | ppl    13.31\n",
      "| epoch  10 |  1100/ 2271 batches | lr 20.00 | loss  2.59 | ppl    13.31\n",
      "| epoch  10 |  1200/ 2271 batches | lr 20.00 | loss  2.60 | ppl    13.49\n",
      "| epoch  10 |  1300/ 2271 batches | lr 20.00 | loss  2.62 | ppl    13.80\n",
      "| epoch  10 |  1400/ 2271 batches | lr 20.00 | loss  2.50 | ppl    12.21\n",
      "| epoch  10 |  1500/ 2271 batches | lr 20.00 | loss  2.56 | ppl    12.95\n",
      "| epoch  10 |  1600/ 2271 batches | lr 20.00 | loss  2.60 | ppl    13.52\n",
      "| epoch  10 |  1700/ 2271 batches | lr 20.00 | loss  2.60 | ppl    13.45\n",
      "| epoch  10 |  1800/ 2271 batches | lr 20.00 | loss  2.59 | ppl    13.28\n",
      "| epoch  10 |  1900/ 2271 batches | lr 20.00 | loss  2.61 | ppl    13.62\n",
      "| epoch  10 |  2000/ 2271 batches | lr 20.00 | loss  2.60 | ppl    13.43\n",
      "| epoch  10 |  2100/ 2271 batches | lr 20.00 | loss  2.60 | ppl    13.42\n",
      "| epoch  10 |  2200/ 2271 batches | lr 20.00 | loss  2.52 | ppl    12.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | valid loss  2.94 | valid ppl    18.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " int six-character \"c99-stdint-7.c\"\n",
      "/* { dg-do compile } */\n",
      "/* { dg-options \"-O -fpic\" } */\n",
      "\n",
      "/* This test should succeed on both\n",
      "32- and 64-bit configurations.  */\n",
      "#include <altivec.h>\n",
      "\n",
      "__vector unsigned char\n",
      "doAbsoluteDifferenceUnsigned (__vector unsigned int *p,\n",
      "\t\t\t\t       __vector\n",
      "unsigned int *p,\n",
      "\t\t\t      __vector unsigned  \n",
      "\n",
      "| epoch  11 |   100/ 2271 batches | lr 20.00 | loss  2.52 | ppl    12.44\n",
      "| epoch  11 |   200/ 2271 batches | lr 20.00 | loss  2.49 | ppl    12.10\n",
      "| epoch  11 |   300/ 2271 batches | lr 20.00 | loss  2.47 | ppl    11.86\n",
      "| epoch  11 |   400/ 2271 batches | lr 20.00 | loss  2.51 | ppl    12.27\n",
      "| epoch  11 |   500/ 2271 batches | lr 20.00 | loss  2.45 | ppl    11.56\n",
      "| epoch  11 |   600/ 2271 batches | lr 20.00 | loss  2.48 | ppl    11.90\n",
      "| epoch  11 |   700/ 2271 batches | lr 20.00 | loss  2.48 | ppl    11.89\n",
      "| epoch  11 |   800/ 2271 batches | lr 20.00 | loss  2.49 | ppl    12.11\n",
      "| epoch  11 |   900/ 2271 batches | lr 20.00 | loss  2.51 | ppl    12.32\n",
      "| epoch  11 |  1000/ 2271 batches | lr 20.00 | loss  2.49 | ppl    12.11\n",
      "| epoch  11 |  1100/ 2271 batches | lr 20.00 | loss  2.50 | ppl    12.13\n",
      "| epoch  11 |  1200/ 2271 batches | lr 20.00 | loss  2.51 | ppl    12.31\n",
      "| epoch  11 |  1300/ 2271 batches | lr 20.00 | loss  2.53 | ppl    12.54\n",
      "| epoch  11 |  1400/ 2271 batches | lr 20.00 | loss  2.41 | ppl    11.12\n",
      "| epoch  11 |  1500/ 2271 batches | lr 20.00 | loss  2.47 | ppl    11.83\n",
      "| epoch  11 |  1600/ 2271 batches | lr 20.00 | loss  2.51 | ppl    12.32\n",
      "| epoch  11 |  1700/ 2271 batches | lr 20.00 | loss  2.51 | ppl    12.28\n",
      "| epoch  11 |  1800/ 2271 batches | lr 20.00 | loss  2.50 | ppl    12.18\n",
      "| epoch  11 |  1900/ 2271 batches | lr 20.00 | loss  2.52 | ppl    12.41\n",
      "| epoch  11 |  2000/ 2271 batches | lr 20.00 | loss  2.51 | ppl    12.27\n",
      "| epoch  11 |  2100/ 2271 batches | lr 20.00 | loss  2.51 | ppl    12.31\n",
      "| epoch  11 |  2200/ 2271 batches | lr 20.00 | loss  2.44 | ppl    11.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | valid loss  2.91 | valid ppl    18.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " } */\n",
      "/* { dg-do run } */\n",
      "/* { dg-options \"-O2 -mavx512dq -mavx512vl\" } */\n",
      "/* { dg-require-effective-target avx512vl } */\n",
      "/* {\n",
      "dg-require-effective-target avx512dq } */\n",
      "\n",
      "#define AVX512VL\n",
      "#define AVX512F_LEN 256\n",
      "#define AVX512F_LEN_HALF 128\n",
      "#include \"avx512f-vprorvd-2.c\"\n",
      "\n",
      "#undef AVX512F_LEN\n",
      "#undef AVX512F_LEN_HALF\n",
      "\n",
      "#define AVX512F_LEN 128\n",
      "#define AVX512F_LEN_HALF 128\n",
      "#include \"avx512f-vpmulld-2.c\"\n",
      "/* Check that the\n",
      "MMIX displacement optimization are used, specifying \n",
      "   __attribute__((mode(SI)));\n",
      "\n",
      "typedef  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  12 |   100/ 2271 batches | lr 20.00 | loss  2.43 | ppl    11.41\n",
      "| epoch  12 |   200/ 2271 batches | lr 20.00 | loss  2.41 | ppl    11.10\n",
      "| epoch  12 |   300/ 2271 batches | lr 20.00 | loss  2.38 | ppl    10.86\n",
      "| epoch  12 |   400/ 2271 batches | lr 20.00 | loss  2.41 | ppl    11.16\n",
      "| epoch  12 |   500/ 2271 batches | lr 20.00 | loss  2.36 | ppl    10.63\n",
      "| epoch  12 |   600/ 2271 batches | lr 20.00 | loss  2.39 | ppl    10.87\n",
      "| epoch  12 |   700/ 2271 batches | lr 20.00 | loss  2.38 | ppl    10.85\n",
      "| epoch  12 |   800/ 2271 batches | lr 20.00 | loss  2.40 | ppl    11.06\n",
      "| epoch  12 |   900/ 2271 batches | lr 20.00 | loss  2.43 | ppl    11.33\n",
      "| epoch  12 |  1000/ 2271 batches | lr 20.00 | loss  2.41 | ppl    11.11\n",
      "| epoch  12 |  1100/ 2271 batches | lr 20.00 | loss  2.41 | ppl    11.18\n",
      "| epoch  12 |  1200/ 2271 batches | lr 20.00 | loss  2.43 | ppl    11.32\n",
      "| epoch  12 |  1300/ 2271 batches | lr 20.00 | loss  2.44 | ppl    11.51\n",
      "| epoch  12 |  1400/ 2271 batches | lr 20.00 | loss  2.33 | ppl    10.24\n",
      "| epoch  12 |  1500/ 2271 batches | lr 20.00 | loss  2.38 | ppl    10.84\n",
      "| epoch  12 |  1600/ 2271 batches | lr 20.00 | loss  2.42 | ppl    11.27\n",
      "| epoch  12 |  1700/ 2271 batches | lr 20.00 | loss  2.42 | ppl    11.22\n",
      "| epoch  12 |  1800/ 2271 batches | lr 20.00 | loss  2.41 | ppl    11.15\n",
      "| epoch  12 |  1900/ 2271 batches | lr 20.00 | loss  2.43 | ppl    11.37\n",
      "| epoch  12 |  2000/ 2271 batches | lr 20.00 | loss  2.43 | ppl    11.35\n",
      "| epoch  12 |  2100/ 2271 batches | lr 20.00 | loss  2.42 | ppl    11.21\n",
      "| epoch  12 |  2200/ 2271 batches | lr 20.00 | loss  2.35 | ppl    10.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | valid loss  2.90 | valid ppl    18.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " unsigned 1st of C99.  */\n",
      "/* { dg-do compile } */\n",
      "/* { dg-options \"-march=octeon -mgp64\" } */\n",
      "/* { dg-skip-if \"code\n",
      "quality test\" { *-*-* } { \"-O0\" } { \"\" } } */\n",
      "/* { dg-final { scan-assembler \"movt|tst|negc|extu\" } }\n",
      "*/\n",
      "\n",
      "int bar (int);\n",
      "int bar (void) initialized, }\n",
      "\n",
      "template<class fn1(void) { }\n",
      "\n",
      "/*  \n",
      "\n",
      "| epoch  13 |   100/ 2271 batches | lr 20.00 | loss  2.35 | ppl    10.47\n",
      "| epoch  13 |   200/ 2271 batches | lr 20.00 | loss  2.32 | ppl    10.13\n",
      "| epoch  13 |   300/ 2271 batches | lr 20.00 | loss  2.31 | ppl    10.04\n",
      "| epoch  13 |   400/ 2271 batches | lr 20.00 | loss  2.33 | ppl    10.30\n",
      "| epoch  13 |   500/ 2271 batches | lr 20.00 | loss  2.28 | ppl     9.73\n",
      "| epoch  13 |   600/ 2271 batches | lr 20.00 | loss  2.31 | ppl    10.03\n",
      "| epoch  13 |   700/ 2271 batches | lr 20.00 | loss  2.31 | ppl    10.08\n",
      "| epoch  13 |   800/ 2271 batches | lr 20.00 | loss  2.32 | ppl    10.19\n",
      "| epoch  13 |   900/ 2271 batches | lr 20.00 | loss  2.34 | ppl    10.38\n",
      "| epoch  13 |  1000/ 2271 batches | lr 20.00 | loss  2.33 | ppl    10.29\n",
      "| epoch  13 |  1100/ 2271 batches | lr 20.00 | loss  2.33 | ppl    10.33\n",
      "| epoch  13 |  1200/ 2271 batches | lr 20.00 | loss  2.35 | ppl    10.52\n",
      "| epoch  13 |  1300/ 2271 batches | lr 20.00 | loss  2.36 | ppl    10.63\n",
      "| epoch  13 |  1400/ 2271 batches | lr 20.00 | loss  2.25 | ppl     9.52\n",
      "| epoch  13 |  1500/ 2271 batches | lr 20.00 | loss  2.31 | ppl    10.05\n",
      "| epoch  13 |  1600/ 2271 batches | lr 20.00 | loss  2.34 | ppl    10.40\n",
      "| epoch  13 |  1700/ 2271 batches | lr 20.00 | loss  2.34 | ppl    10.40\n",
      "| epoch  13 |  1800/ 2271 batches | lr 20.00 | loss  2.33 | ppl    10.31\n",
      "| epoch  13 |  1900/ 2271 batches | lr 20.00 | loss  2.35 | ppl    10.52\n",
      "| epoch  13 |  2000/ 2271 batches | lr 20.00 | loss  2.34 | ppl    10.43\n",
      "| epoch  13 |  2100/ 2271 batches | lr 20.00 | loss  2.34 | ppl    10.37\n",
      "| epoch  13 |  2200/ 2271 batches | lr 20.00 | loss  2.26 | ppl     9.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | valid loss  2.88 | valid ppl    17.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " -mfloat-abi=hard ia64-*-hpux* {\\mlxv\\M}   (ex1->c } } Extracted int tyes and p+1;\n",
      " */\n",
      "/* { dg-do compile } */\n",
      "/* {\n",
      "dg-options \"-O2 -gdwarf -dA -fno-merge-debug-strings\" } */\n",
      "\n",
      "float max[] (uint32_t)0x00ffff00UL) { dg-error \"parse */\n",
      "ARG(int32x4x3_t, 0xf2, ')' no. operator\" } */\n",
      "#endif\n",
      "\n",
      "#if init_data\n",
      "main() { return 0; (\"abcde\", vg;\n",
      "volatile (result1, f4(int (void);\n",
      "\n",
      "enum S  \n",
      "\n",
      "| epoch  14 |   100/ 2271 batches | lr 20.00 | loss  2.27 | ppl     9.65\n",
      "| epoch  14 |   200/ 2271 batches | lr 20.00 | loss  2.24 | ppl     9.41\n",
      "| epoch  14 |   300/ 2271 batches | lr 20.00 | loss  2.23 | ppl     9.26\n",
      "| epoch  14 |   400/ 2271 batches | lr 20.00 | loss  2.26 | ppl     9.55\n",
      "| epoch  14 |   500/ 2271 batches | lr 20.00 | loss  2.20 | ppl     9.03\n",
      "| epoch  14 |   600/ 2271 batches | lr 20.00 | loss  2.23 | ppl     9.31\n",
      "| epoch  14 |   700/ 2271 batches | lr 20.00 | loss  2.24 | ppl     9.35\n",
      "| epoch  14 |   800/ 2271 batches | lr 20.00 | loss  2.25 | ppl     9.46\n",
      "| epoch  14 |   900/ 2271 batches | lr 20.00 | loss  2.27 | ppl     9.64\n",
      "| epoch  14 |  1000/ 2271 batches | lr 20.00 | loss  2.25 | ppl     9.49\n",
      "| epoch  14 |  1100/ 2271 batches | lr 20.00 | loss  2.26 | ppl     9.56\n",
      "| epoch  14 |  1200/ 2271 batches | lr 20.00 | loss  2.28 | ppl     9.74\n",
      "| epoch  14 |  1300/ 2271 batches | lr 20.00 | loss  2.29 | ppl     9.89\n",
      "| epoch  14 |  1400/ 2271 batches | lr 20.00 | loss  2.17 | ppl     8.78\n",
      "| epoch  14 |  1500/ 2271 batches | lr 20.00 | loss  2.23 | ppl     9.33\n",
      "| epoch  14 |  1600/ 2271 batches | lr 20.00 | loss  2.26 | ppl     9.62\n",
      "| epoch  14 |  1700/ 2271 batches | lr 20.00 | loss  2.26 | ppl     9.58\n",
      "| epoch  14 |  1800/ 2271 batches | lr 20.00 | loss  2.27 | ppl     9.64\n",
      "| epoch  14 |  1900/ 2271 batches | lr 20.00 | loss  2.28 | ppl     9.76\n",
      "| epoch  14 |  2000/ 2271 batches | lr 20.00 | loss  2.27 | ppl     9.70\n",
      "| epoch  14 |  2100/ 2271 batches | lr 20.00 | loss  2.27 | ppl     9.63\n",
      "| epoch  14 |  2200/ 2271 batches | lr 20.00 | loss  2.19 | ppl     8.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | valid loss  2.87 | valid ppl    17.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " (tests[0]); w;\n",
      "L:\n",
      "  v3;\n",
      "int (memory_order_relaxed);\n",
      "  ycount)\n",
      "{\n",
      "  \"\\tcld\"  \"__builtin_memset __builtin_apply_args(), *phoff)\n",
      "{\n",
      "  0x1234567;\n",
      "}\n",
      "\n",
      "__uint128_t\n",
      "bar vna1 = (Foo){1, d->a[d->b]->b =\n",
      "{{0, \"string1\";\n",
      "\n",
      "char* start;\n",
      "\n",
      "  value;\n",
      "#if 0x36, LONG_MIN, 40);\n",
      "  if (inside_main)\n",
      "    abort ();\n",
      "  return 0;\n",
      "}\n",
      "/* {\n",
      "dg-do compile } */\n",
      "\n",
      "typedef __PTRDIFF_TYPE__ foo __builtin_avr_wdr __attribute__((persistent)) int x;\n",
      "\n",
      "void  \n",
      "\n",
      "| epoch  15 |   100/ 2271 batches | lr 20.00 | loss  2.20 | ppl     8.99\n",
      "| epoch  15 |   200/ 2271 batches | lr 20.00 | loss  2.17 | ppl     8.72\n",
      "| epoch  15 |   300/ 2271 batches | lr 20.00 | loss  2.16 | ppl     8.65\n",
      "| epoch  15 |   400/ 2271 batches | lr 20.00 | loss  2.18 | ppl     8.83\n",
      "| epoch  15 |   500/ 2271 batches | lr 20.00 | loss  2.14 | ppl     8.52\n",
      "| epoch  15 |   600/ 2271 batches | lr 20.00 | loss  2.16 | ppl     8.69\n",
      "| epoch  15 |   700/ 2271 batches | lr 20.00 | loss  2.16 | ppl     8.67\n",
      "| epoch  15 |   800/ 2271 batches | lr 20.00 | loss  2.18 | ppl     8.87\n",
      "| epoch  15 |   900/ 2271 batches | lr 20.00 | loss  2.19 | ppl     8.96\n",
      "| epoch  15 |  1000/ 2271 batches | lr 20.00 | loss  2.18 | ppl     8.88\n",
      "| epoch  15 |  1100/ 2271 batches | lr 20.00 | loss  2.18 | ppl     8.88\n",
      "| epoch  15 |  1200/ 2271 batches | lr 20.00 | loss  2.22 | ppl     9.16\n",
      "| epoch  15 |  1300/ 2271 batches | lr 20.00 | loss  2.21 | ppl     9.07\n",
      "| epoch  15 |  1400/ 2271 batches | lr 20.00 | loss  2.11 | ppl     8.28\n",
      "| epoch  15 |  1500/ 2271 batches | lr 20.00 | loss  2.17 | ppl     8.78\n",
      "| epoch  15 |  1600/ 2271 batches | lr 20.00 | loss  2.20 | ppl     9.01\n",
      "| epoch  15 |  1700/ 2271 batches | lr 20.00 | loss  2.19 | ppl     8.94\n",
      "| epoch  15 |  1800/ 2271 batches | lr 20.00 | loss  2.20 | ppl     9.00\n",
      "| epoch  15 |  1900/ 2271 batches | lr 20.00 | loss  2.21 | ppl     9.14\n",
      "| epoch  15 |  2000/ 2271 batches | lr 20.00 | loss  2.20 | ppl     9.06\n",
      "| epoch  15 |  2100/ 2271 batches | lr 20.00 | loss  2.19 | ppl     8.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  15 |  2200/ 2271 batches | lr 20.00 | loss  2.13 | ppl     8.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | valid loss  2.86 | valid ppl    17.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " \n",
      "\n",
      "extern int int32_t;\n",
      "typedef int f(vector unsigned int q(void) {\n",
      "   }\n",
      "}\n",
      "f(a){a=(1,1)/2;}\n",
      "/* { dg-do run } */\n",
      "/* { dg-options \"-mpreferred-stack-boundary=4\n",
      "-fno-sanitize-recover=signed-integer-overflow\" } */\n",
      "\n",
      "int\n",
      "main (void)\n",
      "{\n",
      "  long a = -42;\n",
      "  if b < a)\n",
      "    {\n",
      "  \n",
      "   int d = 3;\n",
      "     \n",
      "\n",
      "| epoch  16 |   100/ 2271 batches | lr 20.00 | loss  2.13 | ppl     8.40\n",
      "| epoch  16 |   200/ 2271 batches | lr 20.00 | loss  2.10 | ppl     8.13\n",
      "| epoch  16 |   300/ 2271 batches | lr 20.00 | loss  2.09 | ppl     8.09\n",
      "| epoch  16 |   400/ 2271 batches | lr 20.00 | loss  2.11 | ppl     8.26\n",
      "| epoch  16 |   500/ 2271 batches | lr 20.00 | loss  2.08 | ppl     7.98\n",
      "| epoch  16 |   600/ 2271 batches | lr 20.00 | loss  2.09 | ppl     8.12\n",
      "| epoch  16 |   700/ 2271 batches | lr 20.00 | loss  2.10 | ppl     8.16\n",
      "| epoch  16 |   800/ 2271 batches | lr 20.00 | loss  2.11 | ppl     8.29\n",
      "| epoch  16 |   900/ 2271 batches | lr 20.00 | loss  2.13 | ppl     8.43\n",
      "| epoch  16 |  1000/ 2271 batches | lr 20.00 | loss  2.11 | ppl     8.24\n",
      "| epoch  16 |  1100/ 2271 batches | lr 20.00 | loss  2.12 | ppl     8.32\n",
      "| epoch  16 |  1200/ 2271 batches | lr 20.00 | loss  2.14 | ppl     8.53\n",
      "| epoch  16 |  1300/ 2271 batches | lr 20.00 | loss  2.14 | ppl     8.52\n",
      "| epoch  16 |  1400/ 2271 batches | lr 20.00 | loss  2.04 | ppl     7.71\n",
      "| epoch  16 |  1500/ 2271 batches | lr 20.00 | loss  2.11 | ppl     8.22\n",
      "| epoch  16 |  1600/ 2271 batches | lr 20.00 | loss  2.12 | ppl     8.35\n",
      "| epoch  16 |  1700/ 2271 batches | lr 20.00 | loss  2.12 | ppl     8.34\n",
      "| epoch  16 |  1800/ 2271 batches | lr 20.00 | loss  2.13 | ppl     8.41\n",
      "| epoch  16 |  1900/ 2271 batches | lr 20.00 | loss  2.14 | ppl     8.51\n",
      "| epoch  16 |  2000/ 2271 batches | lr 20.00 | loss  2.13 | ppl     8.43\n",
      "| epoch  16 |  2100/ 2271 batches | lr 20.00 | loss  2.13 | ppl     8.42\n",
      "| epoch  16 |  2200/ 2271 batches | lr 20.00 | loss  2.07 | ppl     7.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | valid loss  2.85 | valid ppl    17.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " } */\n",
      "/* { dg-bogus \" spec_fd[fd].len { target *-*-* } .-1 } */\n",
      "int __attribute__ ((noinline))\n",
      "bar (0x8000000UL ?  : .\\\\\\*..int.'\"\n",
      "stop = (\"xmm18\");\n",
      "  ;\n",
      "  q = 1 == 0; /* { dg-warning \"conversion\" } */\n",
      "}\n",
      "\n",
      "/* { dg-do compile\n",
      "} */\n",
      "/* { dg-options \"-std=iso9899:1990 -pedantic-errors\" } */\n",
      "\n",
      "/* This test  \n",
      "\n",
      "| epoch  17 |   100/ 2271 batches | lr 20.00 | loss  2.07 | ppl     7.91\n",
      "| epoch  17 |   200/ 2271 batches | lr 20.00 | loss  2.03 | ppl     7.64\n",
      "| epoch  17 |   300/ 2271 batches | lr 20.00 | loss  2.03 | ppl     7.58\n",
      "| epoch  17 |   400/ 2271 batches | lr 20.00 | loss  2.04 | ppl     7.73\n",
      "| epoch  17 |   500/ 2271 batches | lr 20.00 | loss  2.01 | ppl     7.43\n",
      "| epoch  17 |   600/ 2271 batches | lr 20.00 | loss  2.04 | ppl     7.66\n",
      "| epoch  17 |   700/ 2271 batches | lr 20.00 | loss  2.03 | ppl     7.64\n",
      "| epoch  17 |   800/ 2271 batches | lr 20.00 | loss  2.05 | ppl     7.76\n",
      "| epoch  17 |   900/ 2271 batches | lr 20.00 | loss  2.07 | ppl     7.94\n",
      "| epoch  17 |  1000/ 2271 batches | lr 20.00 | loss  2.05 | ppl     7.79\n",
      "| epoch  17 |  1100/ 2271 batches | lr 20.00 | loss  2.06 | ppl     7.84\n",
      "| epoch  17 |  1200/ 2271 batches | lr 20.00 | loss  2.08 | ppl     7.98\n",
      "| epoch  17 |  1300/ 2271 batches | lr 20.00 | loss  2.08 | ppl     8.03\n",
      "| epoch  17 |  1400/ 2271 batches | lr 20.00 | loss  1.99 | ppl     7.29\n",
      "| epoch  17 |  1500/ 2271 batches | lr 20.00 | loss  2.04 | ppl     7.70\n",
      "| epoch  17 |  1600/ 2271 batches | lr 20.00 | loss  2.06 | ppl     7.88\n",
      "| epoch  17 |  1700/ 2271 batches | lr 20.00 | loss  2.06 | ppl     7.81\n",
      "| epoch  17 |  1800/ 2271 batches | lr 20.00 | loss  2.07 | ppl     7.92\n",
      "| epoch  17 |  1900/ 2271 batches | lr 20.00 | loss  2.08 | ppl     8.04\n",
      "| epoch  17 |  2000/ 2271 batches | lr 20.00 | loss  2.08 | ppl     7.99\n",
      "| epoch  17 |  2100/ 2271 batches | lr 20.00 | loss  2.07 | ppl     7.91\n",
      "| epoch  17 |  2200/ 2271 batches | lr 20.00 | loss  2.00 | ppl     7.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | valid loss  2.85 | valid ppl    17.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " r4\" } } */\n",
      "/* { dg-final { scan-assembler \"udivsi3_i4\\n\" } } */\n",
      "\n",
      "__vector short long\n",
      "crc32d (unsigned short x, short y)\n",
      "{\n",
      " \n",
      "return _bzhi_u32 (x, y);\n",
      "}\n",
      "\n",
      "unsigned test_mrc2 (unsigned x, unsigned char y)\n",
      "{\n",
      "  return _bzhi_u32 (x, y);\n",
      "}\n",
      "\n",
      "unsigned int\n",
      "crc32d (unsigned int x, unsigned\n",
      "int y)\n",
      "{\n",
      "  return _bzhi_u32 (x, y);\n",
      "}\n",
      "\n",
      "unsigned int\n",
      "crc32w (unsigned int  \n",
      "\n",
      "| epoch  18 |   100/ 2271 batches | lr 20.00 | loss  2.00 | ppl     7.39\n",
      "| epoch  18 |   200/ 2271 batches | lr 20.00 | loss  1.97 | ppl     7.20\n",
      "| epoch  18 |   300/ 2271 batches | lr 20.00 | loss  1.97 | ppl     7.14\n",
      "| epoch  18 |   400/ 2271 batches | lr 20.00 | loss  1.99 | ppl     7.32\n",
      "| epoch  18 |   500/ 2271 batches | lr 20.00 | loss  1.95 | ppl     7.02\n",
      "| epoch  18 |   600/ 2271 batches | lr 20.00 | loss  1.97 | ppl     7.20\n",
      "| epoch  18 |   700/ 2271 batches | lr 20.00 | loss  1.98 | ppl     7.27\n",
      "| epoch  18 |   800/ 2271 batches | lr 20.00 | loss  1.99 | ppl     7.33\n",
      "| epoch  18 |   900/ 2271 batches | lr 20.00 | loss  2.00 | ppl     7.40\n",
      "| epoch  18 |  1000/ 2271 batches | lr 20.00 | loss  2.00 | ppl     7.37\n",
      "| epoch  18 |  1100/ 2271 batches | lr 20.00 | loss  2.00 | ppl     7.42\n",
      "| epoch  18 |  1200/ 2271 batches | lr 20.00 | loss  2.02 | ppl     7.56\n",
      "| epoch  18 |  1300/ 2271 batches | lr 20.00 | loss  2.02 | ppl     7.52\n",
      "| epoch  18 |  1400/ 2271 batches | lr 20.00 | loss  1.92 | ppl     6.82\n",
      "| epoch  18 |  1500/ 2271 batches | lr 20.00 | loss  2.00 | ppl     7.36\n",
      "| epoch  18 |  1600/ 2271 batches | lr 20.00 | loss  2.01 | ppl     7.43\n",
      "| epoch  18 |  1700/ 2271 batches | lr 20.00 | loss  2.00 | ppl     7.37\n",
      "| epoch  18 |  1800/ 2271 batches | lr 20.00 | loss  2.01 | ppl     7.46\n",
      "| epoch  18 |  1900/ 2271 batches | lr 20.00 | loss  2.02 | ppl     7.57\n",
      "| epoch  18 |  2000/ 2271 batches | lr 20.00 | loss  2.02 | ppl     7.52\n",
      "| epoch  18 |  2100/ 2271 batches | lr 20.00 | loss  2.01 | ppl     7.46\n",
      "| epoch  18 |  2200/ 2271 batches | lr 20.00 | loss  1.94 | ppl     6.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | valid loss  2.85 | valid ppl    17.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " -1);\n",
      "  /* { dg-error \"lane -1 out of range 0 - 3\" \"\" {target *-*-*} 0 } */\n",
      " \n",
      "vst3_lane_s64 (int32x2_a, int32x4_b, -1);\n",
      "  /* { dg-error \"lane 2 out of range 0 - 1\" \"\" {target *-*-*} 0\n",
      "} */\n",
      "  vst4q_lane_p8 (int16_a, int16x4_b, -1);\n",
      "  /* {  \n",
      "\n",
      "| epoch  19 |   100/ 2271 batches | lr 20.00 | loss  1.95 | ppl     7.00\n",
      "| epoch  19 |   200/ 2271 batches | lr 20.00 | loss  1.92 | ppl     6.82\n",
      "| epoch  19 |   300/ 2271 batches | lr 20.00 | loss  1.91 | ppl     6.74\n",
      "| epoch  19 |   400/ 2271 batches | lr 20.00 | loss  1.93 | ppl     6.87\n",
      "| epoch  19 |   500/ 2271 batches | lr 20.00 | loss  1.89 | ppl     6.63\n",
      "| epoch  19 |   600/ 2271 batches | lr 20.00 | loss  1.92 | ppl     6.79\n",
      "| epoch  19 |   700/ 2271 batches | lr 20.00 | loss  1.92 | ppl     6.80\n",
      "| epoch  19 |   800/ 2271 batches | lr 20.00 | loss  1.94 | ppl     6.93\n",
      "| epoch  19 |   900/ 2271 batches | lr 20.00 | loss  1.95 | ppl     7.05\n",
      "| epoch  19 |  1000/ 2271 batches | lr 20.00 | loss  1.94 | ppl     6.96\n",
      "| epoch  19 |  1100/ 2271 batches | lr 20.00 | loss  1.94 | ppl     6.97\n",
      "| epoch  19 |  1200/ 2271 batches | lr 20.00 | loss  1.96 | ppl     7.12\n",
      "| epoch  19 |  1300/ 2271 batches | lr 20.00 | loss  1.96 | ppl     7.11\n",
      "| epoch  19 |  1400/ 2271 batches | lr 20.00 | loss  1.87 | ppl     6.50\n",
      "| epoch  19 |  1500/ 2271 batches | lr 20.00 | loss  1.93 | ppl     6.92\n",
      "| epoch  19 |  1600/ 2271 batches | lr 20.00 | loss  1.95 | ppl     7.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  19 |  1700/ 2271 batches | lr 20.00 | loss  1.94 | ppl     6.93\n",
      "| epoch  19 |  1800/ 2271 batches | lr 20.00 | loss  1.96 | ppl     7.08\n",
      "| epoch  19 |  1900/ 2271 batches | lr 20.00 | loss  1.97 | ppl     7.15\n",
      "| epoch  19 |  2000/ 2271 batches | lr 20.00 | loss  1.96 | ppl     7.11\n",
      "| epoch  19 |  2100/ 2271 batches | lr 20.00 | loss  1.95 | ppl     7.06\n",
      "| epoch  19 |  2200/ 2271 batches | lr 20.00 | loss  1.89 | ppl     6.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | valid loss  2.85 | valid ppl    17.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " logging function addition, signed uint32_t;\n",
      "#endif\n",
      "\n",
      "#define s;\n",
      "}\n",
      "#include Insure abc D\t1.4426950408889634074\n",
      "\n",
      "#define executable addressing \"march\" */\n",
      "\n",
      "/* { dg-do compile } */\n",
      "\n",
      "struct S {\n",
      "int a; char h; };\n",
      "struct S { struct S *i; };\n",
      "void bar (struct S *);\n",
      "extern void void\n",
      "baz (struct S *);\n",
      "static\n",
      "int p5.p_x = GAP] int d3;\n",
      "\n",
      "static inline inline void\n",
      "foo (int  \n",
      "\n",
      "| epoch  20 |   100/ 2271 batches | lr 20.00 | loss  1.89 | ppl     6.62\n",
      "| epoch  20 |   200/ 2271 batches | lr 20.00 | loss  1.86 | ppl     6.44\n",
      "| epoch  20 |   300/ 2271 batches | lr 20.00 | loss  1.86 | ppl     6.44\n",
      "| epoch  20 |   400/ 2271 batches | lr 20.00 | loss  1.88 | ppl     6.53\n",
      "| epoch  20 |   500/ 2271 batches | lr 20.00 | loss  1.84 | ppl     6.30\n",
      "| epoch  20 |   600/ 2271 batches | lr 20.00 | loss  1.86 | ppl     6.44\n",
      "| epoch  20 |   700/ 2271 batches | lr 20.00 | loss  1.87 | ppl     6.48\n",
      "| epoch  20 |   800/ 2271 batches | lr 20.00 | loss  1.88 | ppl     6.55\n",
      "| epoch  20 |   900/ 2271 batches | lr 20.00 | loss  1.89 | ppl     6.63\n",
      "| epoch  20 |  1000/ 2271 batches | lr 20.00 | loss  1.88 | ppl     6.54\n",
      "| epoch  20 |  1100/ 2271 batches | lr 20.00 | loss  1.89 | ppl     6.64\n",
      "| epoch  20 |  1200/ 2271 batches | lr 20.00 | loss  1.91 | ppl     6.74\n",
      "| epoch  20 |  1300/ 2271 batches | lr 20.00 | loss  1.90 | ppl     6.70\n",
      "| epoch  20 |  1400/ 2271 batches | lr 20.00 | loss  1.82 | ppl     6.15\n",
      "| epoch  20 |  1500/ 2271 batches | lr 20.00 | loss  1.88 | ppl     6.58\n",
      "| epoch  20 |  1600/ 2271 batches | lr 20.00 | loss  1.90 | ppl     6.70\n",
      "| epoch  20 |  1700/ 2271 batches | lr 20.00 | loss  1.88 | ppl     6.57\n",
      "| epoch  20 |  1800/ 2271 batches | lr 20.00 | loss  1.90 | ppl     6.69\n",
      "| epoch  20 |  1900/ 2271 batches | lr 20.00 | loss  1.92 | ppl     6.79\n",
      "| epoch  20 |  2000/ 2271 batches | lr 20.00 | loss  1.91 | ppl     6.74\n",
      "| epoch  20 |  2100/ 2271 batches | lr 20.00 | loss  1.90 | ppl     6.68\n",
      "| epoch  20 |  2200/ 2271 batches | lr 20.00 | loss  1.84 | ppl     6.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | valid loss  2.85 | valid ppl    17.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  2\n",
      "int (v3, 0}},\n",
      "  unsigned int 0\n",
      "};\n",
      "\n",
      "void\n",
      "int32func = 0xfacec0ff;\n",
      "  int d[8];\n",
      "  int j1)\n",
      "{\n",
      "  int f3;\n",
      "} bb5;\n",
      "\n",
      "\n",
      " if (0) {\n",
      "    __builtin_uaddl_overflow = 16-i;\n",
      "    Z1 = sanitizer/56417  }\n",
      "}\n",
      "\n",
      "/* { dg-do\n",
      "compile } */\n",
      "/* { dg-require-effective-target vect_double } */\n",
      "\n",
      "#define NUM_ELEMS(TYPE) 32\n",
      "\n",
      "/*  \n",
      "\n",
      "| epoch  21 |   100/ 2271 batches | lr 5.00 | loss  1.84 | ppl     6.28\n",
      "| epoch  21 |   200/ 2271 batches | lr 5.00 | loss  1.81 | ppl     6.08\n",
      "| epoch  21 |   300/ 2271 batches | lr 5.00 | loss  1.79 | ppl     5.96\n",
      "| epoch  21 |   400/ 2271 batches | lr 5.00 | loss  1.79 | ppl     6.01\n",
      "| epoch  21 |   500/ 2271 batches | lr 5.00 | loss  1.76 | ppl     5.79\n",
      "| epoch  21 |   600/ 2271 batches | lr 5.00 | loss  1.78 | ppl     5.91\n",
      "| epoch  21 |   700/ 2271 batches | lr 5.00 | loss  1.76 | ppl     5.83\n",
      "| epoch  21 |   800/ 2271 batches | lr 5.00 | loss  1.77 | ppl     5.88\n",
      "| epoch  21 |   900/ 2271 batches | lr 5.00 | loss  1.79 | ppl     5.99\n",
      "| epoch  21 |  1000/ 2271 batches | lr 5.00 | loss  1.77 | ppl     5.86\n",
      "| epoch  21 |  1100/ 2271 batches | lr 5.00 | loss  1.76 | ppl     5.84\n",
      "| epoch  21 |  1200/ 2271 batches | lr 5.00 | loss  1.78 | ppl     5.90\n",
      "| epoch  21 |  1300/ 2271 batches | lr 5.00 | loss  1.77 | ppl     5.84\n",
      "| epoch  21 |  1400/ 2271 batches | lr 5.00 | loss  1.68 | ppl     5.35\n",
      "| epoch  21 |  1500/ 2271 batches | lr 5.00 | loss  1.73 | ppl     5.67\n",
      "| epoch  21 |  1600/ 2271 batches | lr 5.00 | loss  1.74 | ppl     5.70\n",
      "| epoch  21 |  1700/ 2271 batches | lr 5.00 | loss  1.72 | ppl     5.60\n",
      "| epoch  21 |  1800/ 2271 batches | lr 5.00 | loss  1.73 | ppl     5.67\n",
      "| epoch  21 |  1900/ 2271 batches | lr 5.00 | loss  1.73 | ppl     5.65\n",
      "| epoch  21 |  2000/ 2271 batches | lr 5.00 | loss  1.73 | ppl     5.62\n",
      "| epoch  21 |  2100/ 2271 batches | lr 5.00 | loss  1.71 | ppl     5.51\n",
      "| epoch  21 |  2200/ 2271 batches | lr 5.00 | loss  1.64 | ppl     5.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | valid loss  2.83 | valid ppl    16.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " (signed short)(u1 << 2) ? \"optimized\" : b)\n",
      "void foo (int i) {\n",
      "    return (test);\n",
      "  return 0;\n",
      "}\n",
      "/*\n",
      "PR debug/54551 */\n",
      "/* { dg-do compile { target ia32 } } */\n",
      "/* { dg-options \"-O2\" } */\n",
      "\n",
      "extern void bar (void);\n",
      "\n",
      "void\n",
      "foo\n",
      "(void)\n",
      "{\n",
      "  return bar = bar;\n",
      "}\n",
      "/* { dg-do compile }  \n",
      "\n",
      "| epoch  22 |   100/ 2271 batches | lr 5.00 | loss  1.75 | ppl     5.73\n",
      "| epoch  22 |   200/ 2271 batches | lr 5.00 | loss  1.72 | ppl     5.56\n",
      "| epoch  22 |   300/ 2271 batches | lr 5.00 | loss  1.71 | ppl     5.52\n",
      "| epoch  22 |   400/ 2271 batches | lr 5.00 | loss  1.72 | ppl     5.58\n",
      "| epoch  22 |   500/ 2271 batches | lr 5.00 | loss  1.68 | ppl     5.38\n",
      "| epoch  22 |   600/ 2271 batches | lr 5.00 | loss  1.70 | ppl     5.49\n",
      "| epoch  22 |   700/ 2271 batches | lr 5.00 | loss  1.70 | ppl     5.46\n",
      "| epoch  22 |   800/ 2271 batches | lr 5.00 | loss  1.72 | ppl     5.56\n",
      "| epoch  22 |   900/ 2271 batches | lr 5.00 | loss  1.73 | ppl     5.65\n",
      "| epoch  22 |  1000/ 2271 batches | lr 5.00 | loss  1.71 | ppl     5.52\n",
      "| epoch  22 |  1100/ 2271 batches | lr 5.00 | loss  1.71 | ppl     5.55\n",
      "| epoch  22 |  1200/ 2271 batches | lr 5.00 | loss  1.73 | ppl     5.64\n",
      "| epoch  22 |  1300/ 2271 batches | lr 5.00 | loss  1.72 | ppl     5.57\n",
      "| epoch  22 |  1400/ 2271 batches | lr 5.00 | loss  1.63 | ppl     5.11\n",
      "| epoch  22 |  1500/ 2271 batches | lr 5.00 | loss  1.70 | ppl     5.46\n",
      "| epoch  22 |  1600/ 2271 batches | lr 5.00 | loss  1.69 | ppl     5.44\n",
      "| epoch  22 |  1700/ 2271 batches | lr 5.00 | loss  1.68 | ppl     5.36\n",
      "| epoch  22 |  1800/ 2271 batches | lr 5.00 | loss  1.70 | ppl     5.46\n",
      "| epoch  22 |  1900/ 2271 batches | lr 5.00 | loss  1.70 | ppl     5.47\n",
      "| epoch  22 |  2000/ 2271 batches | lr 5.00 | loss  1.69 | ppl     5.42\n",
      "| epoch  22 |  2100/ 2271 batches | lr 5.00 | loss  1.68 | ppl     5.38\n",
      "| epoch  22 |  2200/ 2271 batches | lr 5.00 | loss  1.61 | ppl     5.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | valid loss  2.82 | valid ppl    16.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " preprocess } */\n",
      "/* { dg-output \"\\[^\\n\\r]*\\\\^\" } */\n",
      "/* { dg-do run } */\n",
      "/* { dg-options \"-O2 \\t\\]add\\[ql\\]\" } */\n",
      "/* {\n",
      "dg-require-effective-target ptr32plus } */\n",
      "/* { dg-require-effective-target label_values } */\n",
      "\n",
      "extern void abort (void);\n",
      "\n",
      "typedef long long T;\n",
      "typedef double type32;\n",
      "#endif\n",
      "\n",
      "type32\n",
      "cksum (const char *s)\n",
      "{\n",
      "\n",
      " const char arg3)\n",
      "{\n",
      "\tstype 0x1p-16384q)\n",
      "    {\n",
      "   \n",
      "\n",
      "| epoch  23 |   100/ 2271 batches | lr 5.00 | loss  1.71 | ppl     5.53\n",
      "| epoch  23 |   200/ 2271 batches | lr 5.00 | loss  1.68 | ppl     5.36\n",
      "| epoch  23 |   300/ 2271 batches | lr 5.00 | loss  1.67 | ppl     5.31\n",
      "| epoch  23 |   400/ 2271 batches | lr 5.00 | loss  1.68 | ppl     5.39\n",
      "| epoch  23 |   500/ 2271 batches | lr 5.00 | loss  1.64 | ppl     5.18\n",
      "| epoch  23 |   600/ 2271 batches | lr 5.00 | loss  1.67 | ppl     5.30\n",
      "| epoch  23 |   700/ 2271 batches | lr 5.00 | loss  1.67 | ppl     5.30\n",
      "| epoch  23 |   800/ 2271 batches | lr 5.00 | loss  1.68 | ppl     5.38\n",
      "| epoch  23 |   900/ 2271 batches | lr 5.00 | loss  1.69 | ppl     5.43\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 50):\n",
    "    train(train_data, train_source_sampler, train_target_sampler)          # train\n",
    "    val_loss = evaluate(val_data, val_source_sampler, val_target_sampler)  # validate\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generate(10000, 1.)\n",
    "t15 = generate(10000, 1.5)\n",
    "t075 = generate(10000, 0.75)\n",
    "with open('./generated075-word.txt', 'w') as outf:\n",
    "    outf.write(t075)\n",
    "with open('./generated1-word.txt', 'w') as outf:\n",
    "    outf.write(t1)\n",
    "with open('./generated15-word.txt', 'w') as outf:\n",
    "    outf.write(t15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60K -rw-rw-r-- 1 evgeny evgeny 57K   9 01:02 generated1-word.txt\n",
      "} } */\n",
      "\n",
      "/* Don't emulated added to token the immediate\"  */\n",
      "union u { float c; int b; };\n",
      "\n",
      "int v;\n",
      "\n",
      "void\n",
      "foo (struct st x, struct T y, struct R z)\n",
      "{\n"
     ]
    }
   ],
   "source": [
    "!ls -lsh generated1-word.txt\n",
    "!head generated1-word.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96K -rw-rw-r-- 1 evgeny evgeny 95K   9 01:02 generated15-word.txt\n",
      "(int \\\\t\\]+\\[^\\{\\n\\]*xmm\\[0-9\\]\\[^\\n\\]*xmm\\[0-9\\]\\[^\\n\\]*{%k\\[1-7\\]}(?:\\n|\\[ = (unsigned long exit(int);\n",
      "\n",
      "unsigned int * num_comp,\n",
      "  \"-Werror=error PR77687: Check straight-line strength reduction for a candidate with\n",
      "a basis\n",
      "   hidden by a phi dependence one\n",
      " if executable defined_inside_sys_hdr   -fdirectives-only. We combine source1.i[1] of\n",
      "y:4;\n",
      "  g_31;\n",
      "short \"%a0\", _mm_cmpeq_sd (__builtin_constant_p (2)));\n"
     ]
    }
   ],
   "source": [
    "!ls -lsh generated15-word.txt\n",
    "!head generated15-word.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(int *d, int *src, int len)\r\n",
      "{\r\n",
      "  int i;\r\n",
      "  for (i = 0; i < size; i++)\r\n",
      "  \r\n",
      " dst[i] = __builtin_fma (a[i], b[i]);\r\n",
      "\r\n",
      "  /* check results:  */\r\n",
      "  for (i = 0; i < N;\r\n",
      "i++)\r\n"
     ]
    }
   ],
   "source": [
    "!head generated075-word.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
